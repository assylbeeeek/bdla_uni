{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 2: Analyzing Call Records with Apache Spark\n",
    "Objective: In this task, you will work with a dataset of call records to analyze call patterns using Apache Spark. This exercise will familiarize you with data preprocessing and basic data analysis using Spark's capabilities.\n",
    "Dataset: You should create dataset of call records in a CSV format (at least 1000 records). You can use websites as https://www.mockaroo.com/ , or you can create it by yourself, using PySpark.\n",
    "Instructions:\n",
    "1.\tData Loading: (10%)\n",
    "•\tCreate a dataset with information about ID of a caller, ID of a receiver, duration of a call, call timestamp and location. You should find out appropriate data type for each variable. (8%)\n",
    "•\tLoad the call records dataset into an RDD or DataFrame in Apache Spark. (2%)\n",
    "2.\tData Exploration: (10%)\n",
    "•\tExplore the dataset to understand its structure and contents. Display sample records to get a sense of the data:\n",
    "1.\tDimensions; (1%)\n",
    "2.\tDatatypes; (2%)\n",
    "3.\tSummary statistics; (3%)\n",
    "4.\tUnique and missing values; (1%)\n",
    "5.\tCorrelation matrix. (3%)\n",
    "3.\tData Preprocessing: (20%)\n",
    "•\tClean the data by handling missing values, if any. \n",
    "•\tConvert the timestamp field to a proper datetime format.\n",
    "•\tExtract relevant features from the data, such as day of the week, time of day, or call duration categories.\n",
    "4.\tData Analysis: (25%)\n",
    "•\tCalculate statistics like the total number of calls, average call duration, and peak call times. (10%)\n",
    "•\tIdentify the most frequent callers and receivers. (5%)\n",
    "•\tFind out if there are any unusual call patterns or anomalies in the data. (10%)\n",
    "5.\tVisualization: (15%)\n",
    "•\tCreate visualizations (e.g., bar charts, line plots) using Spark's built-in visualization libraries or export data for external visualization tools.\n",
    "6.\tConclusion: (20%)\n",
    "•\tSummarize the key insights gained from the data analysis. (10%)\n",
    "•\tWhat patterns or trends did you discover in the call records dataset? (10%)\n",
    "Bonus (8%): Create a csv dataset by using PySpark, there should be unique code written by you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"caller_id\", IntegerType(), True),\n",
    "    StructField(\"receiver_id\", IntegerType(), True),\n",
    "    StructField(\"duration\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datetime\n",
    "\n",
    "# Generate random data for 1000 records\n",
    "data = []\n",
    "for i in range(1000):\n",
    "    caller_id = random.randint(1, 10000)\n",
    "    receiver_id = random.randint(1, 10000)\n",
    "    duration = random.randint(1, 3600)  # Duration in seconds\n",
    "    timestamp = datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 365))\n",
    "    location = random.choice([\"Almaty\", \"New York\", \"London\", \"Paris\", \"Tokyo\", \"Astana\", \"Shymkent\", \"Aktau\", \"Aktobe\", \"Atyrau\", \"Oral\", \"Kyzylorda\", \"Taraz\", \"Turkistan\", \"Karagandy\", \"Pavlodar\", \"Petropavlovsk\", \"Semey\", \"Ust-Kamenogorsk\", \"Talgar\", \"Moscow\", \"Kiev\", \"Kyoto\", \"Beijing\", \"Washingtonn\", \"Lisbon\", \"Madrid\", \"Barcelona\", \"Valencia\", \"Buenos Aires\", \"Rome\", \"Amsterdam\", \"Vienna\", \"Berlin\", \"Sofia\", \"Prague\", \"Stockholm\", \"Buchares\", \"Vatican\", \"Montreal\", \"Monaco\", \"Toronto\", \"Kanberra\", \"Seattle\", \"Ottawa\", \"Mexico City\", \"Rio De Janeiro\", \"Brasilia\", \"New-Delhi\", \"Baku\", \"Berne\", \"Frankfurt\", \"Munich\", \"Dublin\", \"Ankara\", \"Istanbul\", \"Ashhabad\", \"Bangkok\", \"Dushanbe\", \"Lubljiana\", \"Singapore\", \"Belgrad\", \"Er-Riyadh\", \"Warsaw\", \"Panama\", \"Abu Dhabi\", \"Oslo\", \"Kathmandu\", \"Kishinev\", \"Rabat\", \"Kuala Lumpur\", \"Jakarta\", \"Baghdad\", \"Tehran\", \"Amman\", \"Kabul\", \"Yerevan\", \"Tbilisi\", \"Algeria\", \"Dhaka\", \"Caracas\", \"Hanoi\", \"Seoul\", \"Pyongyang\", \"Tallinn\", \"Santiago\", \"Zagreb\", \"Vilnius\", \"Tripoli\", \"Riga\", \"Havana\", \"Beirut\", \"Ulaanbaatar\", \"Luxembourg\", \"Copenhagen\"])\n",
    "\n",
    "    data.append((caller_id, receiver_id, duration, timestamp, location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.csv(\"call_records.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+--------------------+--------------+\n",
      "|caller_id|receiver_id|duration|           timestamp|      location|\n",
      "+---------+-----------+--------+--------------------+--------------+\n",
      "|      523|       8303|    1454|2023-08-31 17:40:...|       Vilnius|\n",
      "|     8450|       7743|    1539|2023-02-14 17:40:...|     Amsterdam|\n",
      "|     8848|       1815|     581|2022-12-30 17:40:...|     Kathmandu|\n",
      "|     1989|       3532|    1996|2023-03-27 17:40:...|     Singapore|\n",
      "|     7980|       4544|    3275|2023-07-26 17:40:...|     New-Delhi|\n",
      "|     2852|       7607|    3381|2023-02-01 17:40:...|         Dhaka|\n",
      "|     3204|       5257|    2388|2023-05-29 17:40:...|      Shymkent|\n",
      "|     7429|       6708|    2688|2023-02-06 17:40:...|         Paris|\n",
      "|     9457|       5453|    1620|2023-01-23 17:40:...|Rio De Janeiro|\n",
      "|     8296|       9888|    1577|2023-09-27 17:40:...|      Montreal|\n",
      "|     8414|       5048|    3391|2023-01-30 17:40:...|         Kabul|\n",
      "|     3755|       2297|    2559|2023-07-08 17:40:...|       Seattle|\n",
      "|     1952|       4261|     354|2023-05-10 17:40:...|       Tallinn|\n",
      "|     7220|       3977|    2218|2023-09-10 17:40:...|       Baghdad|\n",
      "|     5480|       6129|     698|2023-02-28 17:40:...|         Kyoto|\n",
      "|     1181|       9190|     116|2023-09-28 17:40:...|         Paris|\n",
      "|     9314|       1866|     171|2023-02-11 17:40:...|     Kathmandu|\n",
      "|     4010|       3121|    2404|2023-07-15 17:40:...|      Shymkent|\n",
      "|     5271|       3347|     417|2023-07-19 17:40:...|     Frankfurt|\n",
      "|     5759|       3740|     902|2022-12-27 17:40:...|     Singapore|\n",
      "+---------+-----------+--------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"location\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- caller_id: integer (nullable = true)\n",
      " |-- receiver_id: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in 'caller_id': 0\n",
      "Number of null values in 'receiver_id': 0\n",
      "Number of null values in 'duration': 0\n",
      "Number of null values in 'timestamp': 0\n",
      "Number of null values in 'location': 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "null_count1 = df.filter(col(\"caller_id\").isNull()).count()\n",
    "null_count2 = df.filter(col(\"receiver_id\").isNull()).count()\n",
    "null_count3 = df.filter(col(\"duration\").isNull()).count()\n",
    "null_count4 = df.filter(col(\"timestamp\").isNull()).count()\n",
    "null_count5 = df.filter(col(\"location\").isNull()).count()\n",
    "\n",
    "print(\"Number of null values in 'caller_id':\", null_count1)\n",
    "print(\"Number of null values in 'receiver_id':\", null_count2)\n",
    "print(\"Number of null values in 'duration':\", null_count3)\n",
    "print(\"Number of null values in 'timestamp':\", null_count4)\n",
    "print(\"Number of null values in 'location':\", null_count5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 1000\n",
      "Number of columns in the dataset: 5\n",
      "Data types of each column:\n",
      "Summary statistics:\n",
      "+-------+------------------+----------------+-----------------+---------+\n",
      "|summary|         caller_id|     receiver_id|         duration| location|\n",
      "+-------+------------------+----------------+-----------------+---------+\n",
      "|  count|              1000|            1000|             1000|     1000|\n",
      "|   mean|           4909.37|        4972.353|         1820.259|     NULL|\n",
      "| stddev|2842.5928909084823|2898.02140959681|1043.218673682618|     NULL|\n",
      "|    min|                 2|              10|                6|Abu Dhabi|\n",
      "|    max|              9992|            9993|             3599|   Zagreb|\n",
      "+-------+------------------+----------------+-----------------+---------+\n",
      "\n",
      "Number of unique values in 'caller_id': 964\n",
      "Number of unique values in 'receiver_id': 964\n",
      "Number of unique values in 'duration': 873\n",
      "Number of unique values in 'location': 95\n",
      "Correlation matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.02570802127433903"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions\n",
    "print(\"Number of rows in the dataset:\", df.count())\n",
    "print(\"Number of columns in the dataset:\", len(df.columns))\n",
    "\n",
    "# Datatypes\n",
    "print(\"Data types of each column:\")\n",
    "df.dtypes\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Summary statistics:\")\n",
    "df.describe().show()\n",
    "\n",
    "# Unique and missing values\n",
    "print(\"Number of unique values in 'caller_id':\", df.select(\"caller_id\").distinct().count())\n",
    "print(\"Number of unique values in 'receiver_id':\", df.select(\"receiver_id\").distinct().count())\n",
    "print(\"Number of unique values in 'duration':\", df.select(\"duration\").distinct().count())\n",
    "print(\"Number of unique values in 'location':\", df.select(\"location\").distinct().count())\n",
    "\n",
    "# Correlation matrix\n",
    "print(\"Correlation matrix:\")\n",
    "df.stat.corr(\"caller_id\", \"receiver_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Calls: 1000\n",
      "Average Call Duration: 1820.259\n",
      "Peak Call Time: Afternoon\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, when, date_format\n",
    "\n",
    "# ...\n",
    "\n",
    "# Extract relevant features before performing the analysis\n",
    "df = df.withColumn(\"day_of_week\", date_format(\"timestamp\", \"E\"))\n",
    "df = df.withColumn(\"time_of_day\", when(hour(\"timestamp\").between(0, 11), \"Morning\").when(hour(\"timestamp\").between(12, 17), \"Afternoon\").otherwise(\"Evening\"))\n",
    "df = df.withColumn(\"duration_category\", when(col(\"duration\") < 300, \"Short\").when((col(\"duration\") >= 300) & (col(\"duration\") < 900), \"Medium\").otherwise(\"Long\"))\n",
    "\n",
    "# ...\n",
    "\n",
    "# Total number of calls, average call duration, and peak call times\n",
    "total_calls = df.count()\n",
    "average_duration = df.agg({\"duration\": \"mean\"}).collect()[0][0]\n",
    "peak_call_times = df.groupBy(\"time_of_day\").count().orderBy(\"count\", ascending=False).first()[\"time_of_day\"]\n",
    "\n",
    "print(\"Total Number of Calls:\", total_calls)\n",
    "print(\"Average Call Duration:\", average_duration)\n",
    "print(\"Peak Call Time:\", peak_call_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Callers:\n",
      "+---------+-----+\n",
      "|caller_id|count|\n",
      "+---------+-----+\n",
      "|     2114|    3|\n",
      "|     7250|    2|\n",
      "|     3742|    2|\n",
      "|     1952|    2|\n",
      "|     7429|    2|\n",
      "+---------+-----+\n",
      "\n",
      "Most Frequent Receivers:\n",
      "+-----------+-----+\n",
      "|receiver_id|count|\n",
      "+-----------+-----+\n",
      "|       6323|    3|\n",
      "|         49|    2|\n",
      "|       9474|    2|\n",
      "|       9513|    2|\n",
      "|       3740|    2|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Most frequent callers and receivers\n",
    "most_frequent_callers = df.groupBy(\"caller_id\").count().orderBy(\"count\", ascending=False).limit(5)\n",
    "most_frequent_receivers = df.groupBy(\"receiver_id\").count().orderBy(\"count\", ascending=False).limit(5)\n",
    "\n",
    "print(\"Most Frequent Callers:\")\n",
    "most_frequent_callers.show()\n",
    "\n",
    "print(\"Most Frequent Receivers:\")\n",
    "most_frequent_receivers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies (Calls with Duration > 1 hour):\n",
      "+---------+-----------+--------+---------+--------+-----------+-----------+-----------------+\n",
      "|caller_id|receiver_id|duration|timestamp|location|day_of_week|time_of_day|duration_category|\n",
      "+---------+-----------+--------+---------+--------+-----------+-----------+-----------------+\n",
      "+---------+-----------+--------+---------+--------+-----------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifying unusual call patterns or anomalies\n",
    "anomalies = df.filter(col(\"duration\") > 3600)  # Filter calls with duration more than 1 hour\n",
    "print(\"Anomalies (Calls with Duration > 1 hour):\")\n",
    "anomalies.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the distribution of call durations using Spark's built-in histogram function\n",
    "# df.select(\"duration\").rdd.flatMap(lambda x: x).histogram(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
